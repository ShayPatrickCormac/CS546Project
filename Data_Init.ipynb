{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import time\n",
    "import urllib.robotparser\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_WIKI_URL = \"https://minecraft.wiki\"\n",
    "#SEED_URL = f\"{BASE_URL}/Minecraft_Wiki\"\n",
    "SAVE_PATH = \"downloaded_pages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help Functions\n",
    "def write_content(file, section_title, subsection_title, content):\n",
    "    if section_title:\n",
    "        file.write(f\"{section_title}\\n\")\n",
    "    if subsection_title:\n",
    "        file.write(f\"  {subsection_title}\\n\")\n",
    "    if content:\n",
    "        file.write(f\"    {' '.join(content)}\\n\\n\")\n",
    "\n",
    "def write_brew_recipe(cell):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    input_span = cell.select_one('.mcui-input')\n",
    "    if input_span:\n",
    "        input_items = input_span.select('.invslot-item-image a')\n",
    "        for item in input_items:\n",
    "            item_title = item.get('title')\n",
    "            if item_title:\n",
    "                inputs.append(item_title)\n",
    "        input_items = input_span.select('.invslot-item invslot-item-image animated-active a')\n",
    "        for item in input_items:\n",
    "            item_title = item.get('title')\n",
    "            if item_title:\n",
    "                inputs.append(item_title)\n",
    "    output_container = cell.select_one('.mcui-output')\n",
    "    if output_container:\n",
    "        output_items = output_container.select('.invslot-item-image a')\n",
    "        for item in output_items:\n",
    "            item_title = item.get('title')\n",
    "            if item_title:\n",
    "                outputs.append(item_title)\n",
    "    recipe_parts = []\n",
    "    if inputs:\n",
    "        recipe_parts.append(\"input:\" + \"; \".join(inputs))\n",
    "    if outputs:\n",
    "        recipe_parts.append(\"output:\" + \"; \".join(outputs))\n",
    "    return ' | '.join(recipe_parts)\n",
    "\n",
    "def write_with_sprite(cell):\n",
    "    img_tags = cell.find_all('img', class_='pixel-image')\n",
    "    sprite_names = []\n",
    "    for img_tag in img_tags:\n",
    "        alt_text = img_tag.get('alt', '')\n",
    "        sprite_name = alt_text.split(' ')[1]\n",
    "        sprite_name = sprite_name.replace('.png', '').replace('-', ' ').replace(':','')\n",
    "        sprite_names.append(sprite_name)\n",
    "        sprite_names_text = '; '.join(sprite_names)\n",
    "        additional_text = ' '.join(cell.stripped_strings)\n",
    "        combined_text = f\"{sprite_names_text} {additional_text}\".strip()\n",
    "    return combined_text\n",
    "\n",
    "def write_smithing_recipe(cell):\n",
    "    inputs = []\n",
    "    items = cell.select('.invslot-item-image a')\n",
    "    if items:\n",
    "        for item in items:\n",
    "            item_title = item.get('title')\n",
    "            if item_title:\n",
    "                inputs.append(item_title)\n",
    "    output = inputs[-1] \n",
    "    inputs = inputs[:-1]\n",
    "    recipe_parts = []\n",
    "    recipe_parts.append(\"input:\" + \"; \".join(inputs))\n",
    "    recipe_parts.append(\"output:\" + output)\n",
    "    recipe_str = ' | '.join(recipe_parts)\n",
    "    return recipe_str\n",
    "\n",
    "\n",
    "def write_crafting_recipe(cell):\n",
    "    mcui_input = cell.find('span', class_='mcui-input')\n",
    "    if mcui_input:\n",
    "        inv_slots = mcui_input.find_all('span', class_='invslot')\n",
    "        recipe_pattern = []\n",
    "        for slot in inv_slots:\n",
    "            item = slot.find('span', class_='invslot-item')\n",
    "            if item:\n",
    "                title = item.a.get('title') if item.a else ''\n",
    "                recipe_pattern.append(title)\n",
    "            else:\n",
    "                recipe_pattern.append(None)\n",
    "        three_by_three_recipe = [recipe_pattern[i:i+3] for i in range(0, len(recipe_pattern), 3)]\n",
    "        return str(three_by_three_recipe)\n",
    "    return \"\"\n",
    "    \n",
    "\n",
    "def write_table(table, file):\n",
    "    rows = table.find_all('tr')\n",
    "    csv_writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    for row in rows:\n",
    "        row_text = []\n",
    "        for cell in row.find_all(['td','th']):\n",
    "            if cell.find('span', class_='mcui mcui-Brewing_Stand pixel-image'):\n",
    "                cell_text = write_brew_recipe(cell)\n",
    "            elif cell.find('span', class_='mcui mcui-Smithing_Table pixel-image'):\n",
    "                cell_text = write_smithing_recipe(cell)\n",
    "            elif cell.find('span', class_='mcui mcui-Crafting_Table pixel-image'):\n",
    "                cell_text = write_crafting_recipe(cell)\n",
    "            elif cell.get_text(separator=\" \", strip=True) == '' and cell.find('span', class_='sprite-file'):\n",
    "                cell_text = write_with_sprite(cell)\n",
    "            else:\n",
    "                cell_text = cell.get_text(separator=\" \", strip=True)\n",
    "            row_text.append(cell_text)\n",
    "        file.write('    ')\n",
    "        csv_writer.writerow(row_text)\n",
    "    file.write('\\n')\n",
    "\n",
    "\n",
    "def crawl_page(url, save_path, pagename):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    with open(save_path + pagename + \".txt\", 'w', encoding='utf-8') as file:\n",
    "        main_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "        section_title = \"Overview\"\n",
    "        subsection_title = \"\"\n",
    "        content = []\n",
    "        \n",
    "        for element in main_div.find_all(['h2', 'h3', 'dl', 'p', 'table'], recursive=False):\n",
    "            # print(f'Processing {element.name} tag')\n",
    "            if element.name == 'h2':\n",
    "                # Write previous section\n",
    "                write_content(file, section_title, subsection_title, content)\n",
    "                content = []\n",
    "                section_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "                if section_title == \"History\" or section_title == \"Sounds\":\n",
    "                    break\n",
    "                subsection_title = \"\"\n",
    "            elif element.name == 'h3':\n",
    "                # Write previous subsection + content\n",
    "                write_content(file, section_title, subsection_title, content)\n",
    "                content = []\n",
    "                section_title = \"\"\n",
    "                subsection_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "            elif element.name == 'p' or element.name == 'dl':\n",
    "                content.append(element.text.strip())\n",
    "            elif element.name == 'table':\n",
    "                write_content(file, section_title, subsection_title, content)\n",
    "                write_table(element, file)\n",
    "                section_title = \"\"\n",
    "                subsection_title = \"\"\n",
    "                content = []\n",
    "\n",
    "        if content:\n",
    "            write_content(file, section_title, subsection_title, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://minecraft.wiki/w/Block\n"
     ]
    }
   ],
   "source": [
    "# Test revoming Duplicates\n",
    "'''\n",
    "test_url = \"https://minecraft.wiki/w/Blocks\"\n",
    "response = requests.get(test_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Try to get the canonical URL\n",
    "canonical_tag = soup.find(\"link\", rel=\"canonical\")\n",
    "if canonical_tag and canonical_tag.has_attr(\"href\"):\n",
    "    canonical_url = canonical_tag[\"href\"]\n",
    "else:\n",
    "    canonical_url = response.url\n",
    "\n",
    "print(canonical_url)\n",
    "'''\n",
    "def getRealURL(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Try to get the canonical URL\n",
    "    canonical_tag = soup.find(\"link\", rel=\"canonical\")\n",
    "    if canonical_tag and canonical_tag.has_attr(\"href\"):\n",
    "        canonical_url = canonical_tag[\"href\"]\n",
    "    else:\n",
    "        canonical_url = response.url\n",
    "    return canonical_url\n",
    "print(getRealURL(\"https://minecraft.wiki/w/Blocks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working\n",
    "rp = urllib.robotparser.RobotFileParser()\n",
    "rp.set_url(urljoin(BASE_WIKI_URL, \"/robots.txt\"))\n",
    "# print(urljoin(BASE_URL, \"robots.txt\"))\n",
    "rp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass\n",
    "disallowed_keywords = ['File:', 'Special:', 'Property', 'User:', 'Minecraft_Wiki', 'Help:', 'Minecraft_Legends', 'Minecraft_Dungeons', '=', 'Minecraft_Story_Mode', 'Story_Mode',':','edition']\n",
    "def can_fetch(url):\n",
    "    return not any(keyword in url for keyword in disallowed_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# pass\n",
    "print(can_fetch(\"https://minecraft.wiki/w/Trading\"))\n",
    "print(can_fetch(\"https://minecraft.wiki/w/Special:UserLogin?returnto=Player\"))\n",
    "print(can_fetch(\"https://minecraft.wiki/*?title=Property%3A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Gameplay Trading\n",
    "BASE_URL = \"https://minecraft.wiki/w/Trading\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Trading/\"\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "to_visit = {BASE_URL}\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "with open(SAVE_PATH+\"Trading.txt\", 'w', encoding='utf-8') as file:\n",
    "    main_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    section_title = \"Overview\"\n",
    "    subsection_title = \"\"\n",
    "    content = []\n",
    "\n",
    "    def write_content():\n",
    "        if section_title:\n",
    "            file.write(f\"{section_title}\\n\")\n",
    "        if subsection_title:\n",
    "            file.write(f\"  {subsection_title}\\n\")\n",
    "        if content:\n",
    "            file.write(f\"    {' '.join(content)}\\n\\n\")\n",
    "\n",
    "    def write_table(table):\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer, index=False, header=True)\n",
    "        table_str = csv_buffer.getvalue()\n",
    "        indented_table_str = '    ' + table_str.replace('\\n', '\\n    ')\n",
    "        file.write(\"{}\\n\\n\".format(indented_table_str))\n",
    "    \n",
    "    for element in main_div.find_all(['h2', 'h3', 'p', 'table'], recursive=False):\n",
    "        # print(f'Processing {element.name} tag')\n",
    "        if element.name == 'h2':\n",
    "            # Write previous section\n",
    "            write_content()\n",
    "            content = []\n",
    "            section_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "            subsection_title = \"\"\n",
    "        elif element.name == 'h3':\n",
    "            # Write previous subsection content\n",
    "            write_content()\n",
    "            content = []\n",
    "            section_title = \"\"\n",
    "            subsection_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "        elif element.name == 'p':\n",
    "            content.append(element.text.strip())\n",
    "        elif element.name == 'table':\n",
    "            write_content()\n",
    "            write_table(element)\n",
    "            section_title = \"\"\n",
    "            subsection_title = \"\"\n",
    "            content = []\n",
    "\n",
    "    # Write last section\n",
    "    if content:\n",
    "        write_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Gameplay Brewing\n",
    "BASE_URL = \"https://minecraft.wiki/w/Brewing\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Brewing/\"\n",
    "PAGE_NAME = \"Brewing\"\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Gameplay Enchanting\n",
    "BASE_URL = \"https://minecraft.wiki/w/Enchanting\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Enchanting/\"\n",
    "PAGE_NAME = \"Enchanting\"\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "table = soup.find('table', {'data-description': 'Summary of enchantments'})\n",
    "links_to_crawl = []\n",
    "file_names = []\n",
    "for tr in table.find_all('tr')[1:]:\n",
    "    first_link = tr.find('a')\n",
    "    if first_link and first_link.has_attr('href'):\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, first_link['href'])\n",
    "        links_to_crawl.append(full_url)\n",
    "        file_names.append(first_link['title'])\n",
    "for i in range(len(links_to_crawl)):\n",
    "    crawl_page(links_to_crawl[i], SAVE_PATH, file_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_recipe(url, save_path, pagename):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table_body = soup.find('tbody')\n",
    "    recipes = []\n",
    "    rows = table_body.find_all('tr')[1:]\n",
    "    for tr in rows:\n",
    "        columns = tr.find_all(['th', 'td'])\n",
    "        names = [a.get_text(strip=True) for a in columns[0].find_all('a')]\n",
    "        ingredients = [a.get_text(strip=True) for a in columns[1].find_all('a')]\n",
    "        mcui_input = columns[2].find('span', class_='mcui-input')\n",
    "        if mcui_input:\n",
    "            inv_slots = mcui_input.find_all('span', class_='invslot')\n",
    "            recipe_pattern = []\n",
    "            for slot in inv_slots:\n",
    "                item = slot.find('span', class_='invslot-item')\n",
    "                if item:\n",
    "                    title = item.a.get('title') if item.a else ''\n",
    "                    recipe_pattern.append(title)\n",
    "                else:\n",
    "                    recipe_pattern.append(None)\n",
    "            three_by_three_recipe = [recipe_pattern[i:i+3] for i in range(0, len(recipe_pattern), 3)]\n",
    "        # print(three_by_three_recipe)\n",
    "        description = columns[3].get_text(strip=True) if len(columns) > 3 else \"\"\n",
    "\n",
    "        recipe = {\n",
    "                'names': names,\n",
    "                'ingredients': ingredients,\n",
    "                'sample pattern': three_by_three_recipe,\n",
    "                'description': description\n",
    "            }\n",
    "        recipes.append(recipe)\n",
    "    with open(os.path.join(save_path, f\"{pagename}.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(recipes, f, ensure_ascii=False, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://minecraft.wiki/w/Crafting/Building_blocks', 'https://minecraft.wiki/w/Crafting/Decoration_blocks', 'https://minecraft.wiki/w/Crafting/Redstone', 'https://minecraft.wiki/w/Crafting/Transportation', 'https://minecraft.wiki/w/Crafting/Foodstuffs', 'https://minecraft.wiki/w/Crafting/Tools', 'https://minecraft.wiki/w/Crafting/Combat', 'https://minecraft.wiki/w/Crafting/Brewing', 'https://minecraft.wiki/w/Crafting/Materials', 'https://minecraft.wiki/w/Crafting/Miscellaneous']\n"
     ]
    }
   ],
   "source": [
    "# Crawing for Crafting\n",
    "BASE_URL = \"https://minecraft.wiki/w/Crafting\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Crafting/\"\n",
    "PAGE_NAME = \"Crafating\"\n",
    "\n",
    "# Crawing the main page\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "with open(SAVE_PATH + PAGE_NAME + \".txt\", 'w', encoding='utf-8') as file:\n",
    "    main_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    section_title = \"Overview\"\n",
    "    subsection_title = \"\"\n",
    "    content = []\n",
    "    \n",
    "    for element in main_div.find_all(['h2', 'h3', 'p', 'table'], recursive=False):\n",
    "        # print(f'Processing {element.name} tag')\n",
    "        if element.name == 'h2':\n",
    "            # Write previous section\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            content = []\n",
    "            section_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "            if section_title == \"Complete recipe list\":\n",
    "                break\n",
    "            subsection_title = \"\"\n",
    "        elif element.name == 'h3':\n",
    "            # Write previous subsection + content\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            content = []\n",
    "            section_title = \"\"\n",
    "            subsection_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "        elif element.name == 'p':\n",
    "            content.append(element.text.strip())\n",
    "        elif element.name == 'table':\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            write_table(element, file)\n",
    "            section_title = \"\"\n",
    "            subsection_title = \"\"\n",
    "            content = []\n",
    "\n",
    "    if content:\n",
    "        write_content(file, section_title, subsection_title, content)\n",
    "\n",
    "\n",
    "\n",
    "# Crawing the recipes to json files\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "sections = soup.select('div.load-page')\n",
    "titles = []\n",
    "urls = []\n",
    "for section in sections:\n",
    "    headline = section.find(class_='mw-headline')\n",
    "    if headline:\n",
    "        curr_section = headline.text.strip().replace(' ', \"_\")\n",
    "        titles.append(curr_section)\n",
    "        # print(curr_section)\n",
    "        temp = section.find(class_='hatnote searchaux')\n",
    "        first_link = temp.find('a')\n",
    "        if first_link and first_link.has_attr('href'):\n",
    "            full_url = requests.compat.urljoin(BASE_WIKI_URL, first_link['href'])\n",
    "            urls.append(full_url)\n",
    "            # print(full_url)\n",
    "print(urls)\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    crawl_recipe(urls[i], SAVE_PATH, titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Smelting\n",
    "# Commenting out since Smelting needed manual cleaning\n",
    "'''\n",
    "BASE_URL = \"https://minecraft.wiki/w/Smelting\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Smelting/\"\n",
    "PAGE_NAME = \"Smelting\"\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Smithing\n",
    "BASE_URL = \"https://minecraft.wiki/w/Smithing\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Smithing/\"\n",
    "PAGE_NAME = \"Smithing\"\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Archaeology\n",
    "BASE_URL = \"https://minecraft.wiki/w/Archaeology\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Archaeology/\"\n",
    "PAGE_NAME = \"Archaeology\"\n",
    "\n",
    "def crawl_page(url, save_path, pagename):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    with open(save_path + pagename + \".txt\", 'w', encoding='utf-8') as file:\n",
    "        main_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "        section_title = \"Overview\"\n",
    "        subsection_title = \"\"\n",
    "        content = []\n",
    "        \n",
    "        for element in main_div.find_all(['h2', 'h3', 'ul', 'dl','p', 'table'], recursive=False):\n",
    "            # print(f'Processing {element.name} tag')\n",
    "            if element.name == 'h2':\n",
    "                # Write previous section\n",
    "                write_content(file, section_title, subsection_title, content)\n",
    "                content = []\n",
    "                section_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "                if section_title == \"History\" or section_title == \"Sounds\":\n",
    "                    break\n",
    "                subsection_title = \"\"\n",
    "            elif element.name == 'h3':\n",
    "                # Write previous subsection + content\n",
    "                write_content(file, section_title, subsection_title, content)\n",
    "                content = []\n",
    "                section_title = \"\"\n",
    "                subsection_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "            elif element.name == 'p' or element.name == 'ul' or element.name == 'dl':\n",
    "                content.append(element.text.strip())\n",
    "            elif element.name == 'table':\n",
    "                write_content(file, section_title, subsection_title, content)\n",
    "                write_table(element, file)\n",
    "                section_title = \"\"\n",
    "                subsection_title = \"\"\n",
    "                content = []\n",
    "\n",
    "        if content:\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Redstone circuits\n",
    "BASE_URL = \"https://minecraft.wiki/w/Redstone_circuits\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Redstone_circuits/\"\n",
    "PAGE_NAME = \"Redstone_circuits\"\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Effect\n",
    "BASE_URL = \"https://minecraft.wiki/w/Effect\"\n",
    "SAVE_PATH = \"downloaded_pages/Effect/\"\n",
    "PAGE_NAME = \"Effect\"\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "table = soup.find('table', {'data-description': 'Effects'})\n",
    "links_to_crawl = []\n",
    "file_names = []\n",
    "for tr in table.find_all('tr')[1:]:\n",
    "    first_link = tr.find('a')\n",
    "    if first_link and first_link.has_attr('href'):\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, first_link['href'])\n",
    "        links_to_crawl.append(full_url)\n",
    "        file_names.append(first_link['title'])\n",
    "for i in range(len(links_to_crawl)):\n",
    "    crawl_page(links_to_crawl[i], SAVE_PATH, file_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '/w/Acacia_Button': No scheme supplied. Perhaps you meant http:///w/Acacia_Button?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bb762\\Desktop\\temp\\Data_Init.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X25sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         realURL \u001b[39m=\u001b[39m a_tags[\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X25sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmw-redirect\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m a_tags[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m, []):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X25sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m             realURL \u001b[39m=\u001b[39m getRealURL(a_tags[\u001b[39m1\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mhref\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X25sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         to_crawl\u001b[39m.\u001b[39madd(realURL)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X25sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNormal_Blocks.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "\u001b[1;32mc:\\Users\\bb762\\Desktop\\temp\\Data_Init.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetRealURL\u001b[39m(url):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# Try to get the canonical URL\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\requests\\sessions.py:573\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39m# Create the Request.\u001b[39;00m\n\u001b[0;32m    561\u001b[0m req \u001b[39m=\u001b[39m Request(\n\u001b[0;32m    562\u001b[0m     method\u001b[39m=\u001b[39mmethod\u001b[39m.\u001b[39mupper(),\n\u001b[0;32m    563\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    571\u001b[0m     hooks\u001b[39m=\u001b[39mhooks,\n\u001b[0;32m    572\u001b[0m )\n\u001b[1;32m--> 573\u001b[0m prep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_request(req)\n\u001b[0;32m    575\u001b[0m proxies \u001b[39m=\u001b[39m proxies \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m    577\u001b[0m settings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    578\u001b[0m     prep\u001b[39m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    579\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\requests\\sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m     auth \u001b[39m=\u001b[39m get_netrc_auth(request\u001b[39m.\u001b[39murl)\n\u001b[0;32m    483\u001b[0m p \u001b[39m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 484\u001b[0m p\u001b[39m.\u001b[39;49mprepare(\n\u001b[0;32m    485\u001b[0m     method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(),\n\u001b[0;32m    486\u001b[0m     url\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49murl,\n\u001b[0;32m    487\u001b[0m     files\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mfiles,\n\u001b[0;32m    488\u001b[0m     data\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m    489\u001b[0m     json\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mjson,\n\u001b[0;32m    490\u001b[0m     headers\u001b[39m=\u001b[39;49mmerge_setting(\n\u001b[0;32m    491\u001b[0m         request\u001b[39m.\u001b[39;49mheaders, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheaders, dict_class\u001b[39m=\u001b[39;49mCaseInsensitiveDict\n\u001b[0;32m    492\u001b[0m     ),\n\u001b[0;32m    493\u001b[0m     params\u001b[39m=\u001b[39;49mmerge_setting(request\u001b[39m.\u001b[39;49mparams, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams),\n\u001b[0;32m    494\u001b[0m     auth\u001b[39m=\u001b[39;49mmerge_setting(auth, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauth),\n\u001b[0;32m    495\u001b[0m     cookies\u001b[39m=\u001b[39;49mmerged_cookies,\n\u001b[0;32m    496\u001b[0m     hooks\u001b[39m=\u001b[39;49mmerge_hooks(request\u001b[39m.\u001b[39;49mhooks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhooks),\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    498\u001b[0m \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\requests\\models.py:368\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_method(method)\n\u001b[1;32m--> 368\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_url(url, params)\n\u001b[0;32m    369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\requests\\models.py:439\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39m*\u001b[39me\u001b[39m.\u001b[39margs)\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m scheme:\n\u001b[1;32m--> 439\u001b[0m     \u001b[39mraise\u001b[39;00m MissingSchema(\n\u001b[0;32m    440\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No scheme supplied. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPerhaps you meant http://\u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m host:\n\u001b[0;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No host supplied\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL '/w/Acacia_Button': No scheme supplied. Perhaps you meant http:///w/Acacia_Button?"
     ]
    }
   ],
   "source": [
    "# Crawing for Blocks\n",
    "BASE_URL = \"https://minecraft.wiki/w/Block\"\n",
    "SAVE_PATH = \"downloaded_pages/Block/\"\n",
    "PAGE_NAME = \"Block\"\n",
    "\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "with open(SAVE_PATH + PAGE_NAME + \".txt\", 'w', encoding='utf-8') as file:\n",
    "    main_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    section_title = \"Overview\"\n",
    "    subsection_title = \"\"\n",
    "    content = []\n",
    "    \n",
    "    for element in main_div.find_all(['h2', 'h3', 'p', 'table'], recursive=False):\n",
    "        # print(f'Processing {element.name} tag')\n",
    "        if element.name == 'h2':\n",
    "            # Write previous section\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            content = []\n",
    "            section_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "            if section_title == \"List of blocks\":\n",
    "                break\n",
    "            subsection_title = \"\"\n",
    "        elif element.name == 'h3':\n",
    "            # Write previous subsection + content\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            content = []\n",
    "            section_title = \"\"\n",
    "            subsection_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "        elif element.name == 'p':\n",
    "            content.append(element.text.strip())\n",
    "        elif element.name == 'table':\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            write_table(element, file)\n",
    "            section_title = \"\"\n",
    "            subsection_title = \"\"\n",
    "            content = []\n",
    "\n",
    "    if content:\n",
    "        write_content(file, section_title, subsection_title, content)\n",
    "\n",
    "divs = soup.find_all('div', class_='div-col columns column-width')[:2]\n",
    "names = []\n",
    "to_crawl = set()\n",
    "for li in divs[0].find_all('li'):\n",
    "    a_tags = li.find_all('a')\n",
    "    if len(a_tags) > 1:\n",
    "        name = a_tags[1].get_text(strip=True)\n",
    "        names.append(name)\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, a_tags[1]['href'])\n",
    "        if \"mw-redirect\" in a_tags[1].get('class', []):\n",
    "            full_url = getRealURL(full_url)\n",
    "        to_crawl.add(realURL)\n",
    "with open('Normal_Blocks.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(names, file, indent=4, ensure_ascii=False)\n",
    "SAVE_PATH = \"downloaded_pages/Block/Normal_Blocks/\"\n",
    "for link in to_crawl:\n",
    "    file_name = link.split('/')[-1]\n",
    "    crawl_page(link, SAVE_PATH, file_name)\n",
    "    time.sleep(random(5))\n",
    "\n",
    "names = []\n",
    "to_crawl = set()\n",
    "for li in divs[1].find_all('li'):\n",
    "    a_tags = li.find_all('a')\n",
    "    if len(a_tags) > 1:\n",
    "        name = a_tags[1].get_text(strip=True)\n",
    "        names.append(name)\n",
    "        # realURL = a_tags[1]['href']\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, a_tags[1]['href'])\n",
    "        if \"mw-redirect\" in a_tags[1].get('class', []):\n",
    "            full_url = getRealURL(full_url)\n",
    "        to_crawl.add(realURL)\n",
    "with open('Technical_Blocks.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(names, file, indent=4, ensure_ascii=False)\n",
    "SAVE_PATH = \"downloaded_pages/Block/Technical_Blocks/\"\n",
    "for link in to_crawl:\n",
    "    file_name = link.split('/')[-1]\n",
    "    crawl_page(link, SAVE_PATH, file_name)\n",
    "    time.sleep(random(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling and saving finished!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "while to_visit:\n",
    "    current_url = to_visit.pop()\n",
    "\n",
    "    if current_url in visited_urls or not can_fetch(current_url):\n",
    "        continue\n",
    "\n",
    "    response = requests.get(current_url)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    canonical_tag = soup.find(\"link\", rel=\"canonical\")\n",
    "\n",
    "    if canonical_tag[\"href\"]:\n",
    "        # Check Redirect\n",
    "        if not canonical_tag[\"href\"] == current_url:\n",
    "            visited_urls.add(current_url)\n",
    "            current_url = canonical_tag[\"href\"]\n",
    "            if current_url in visited_urls:\n",
    "                continue\n",
    "\n",
    "    \n",
    "    page_name = urlparse(current_url).path.split('/')[-1] or 'index'\n",
    "    page_name = page_name.replace(':', '_').replace('%', '_').replace('?', '_') + \".html\"\n",
    "    with open(os.path.join(SAVE_PATH, page_name), 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    # Extract links\n",
    "    for link in soup.find_all('a', href=True):\n",
    "            full_url = link['href'] if 'http' in link['href'] else BASE_URL + link['href']\n",
    "            full_url = sanitize_url(full_url)\n",
    "            if not can_fetch(full_url):\n",
    "                continue\n",
    "            if full_url not in visited_urls and BASE_WIKI_URL in full_url:\n",
    "                to_visit.add(full_url)\n",
    "\n",
    "    visited_urls.add(current_url)\n",
    "\n",
    "    time.sleep(random.uniform(1, 4))\n",
    "\n",
    "print(\"Crawling and saving finished!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n",
      "3466\n"
     ]
    }
   ],
   "source": [
    "print('https://minecraft.wiki/w/Acacia_Button' in to_visit)\n",
    "print(len(to_visit))\n",
    "print(len(visited_urls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
